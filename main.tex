\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Pattern Recognition with Decision Trees}
\author{Niclas Jonsson}
\date{\today}

\begin{document}

\maketitle
\section{Introduction}
A decision tree is a tool in Machine Learning and Data Mining used to classify objects into classes\cite{Quinlan1986InductionTrees}. An object is a set of attribution. An example of a classification problem could be to classify if a person has a certain disease or not. A person correspond to an object with the following attributes: gender, age, weight, length and blood pressure where the value of gender is either male of female, and the values for weight, length and blood pressure is a number. To build a decision tree that can be used to determinate if a person is sick or not, a set of objects called the training set where each object's class label is foreknow is given as input to the algorithm which constructs the decision tree\cite{Loh2011ClassificationTrees}.

Let say a decision tree has been built be a training set of patients from a hospital with a certain disease and healthy people without the disease.
Then can this decision tree be used to determinate if a person is suffering from the disease. A graphic representation of a decision tree can be seen in figure \ref{fig:tree}. To determinate if a person is sick or not according to the our model (the decision tree), one can follow the branches from the tree until there is no more branches. Then assign the person the class that is a majority in the node. An assignment of a class to an object is called classification. A classification error is when an object is classified wrongly. For example, if the decision tree classify an object to healthy even due the object correspond to a person which is sick, then the decision tree would have made a classification error.

In today's society is pattern recognition extremely useful. Although there are many tools for pattern recognitions besides decision tree's like bayesian network, decision tree's is a powerful tool that can be used in many areas. The example above illustrate how itn be used in medicine but it can also be applied in many other areas like weather forecast, advertisement and more present-day problems like detecting email spam.

A problem when building a decision tree is that the objects in the training set can contain noise and have no data in some attributes. 
A training set is said to have noise  if a class label in the training set is wrong\cite{Feelders2000MethodologicalMining}. An example of noise in our classification problem is if a patient is sick but someone made a mistake and wrote healthy in the journal so that the object in the training set now has the label healthy. A lot of noise can led to a decision tree that does not only model the underlying structure of the data but the noise of the known data set as well. 

Missing values in some attribute can occur for several reasons. It could be hard to measure and instead of give an uncertain value which could could be noise. As the case for noise, some information could be to sensitive to share and some people might skip answering some questions.

\section{Technical Part}
One approach to find the best tree, i.e. the smallest tree with fewest error classifications would be to generate all possible decision trees and pick the simplest of the once that classify the training set correct. This approach is unsuccessful in practise for objects with many attributes since they number of tree's grows very fast with respect to the number of attributes. Instead of a complete search, other algorithms are used to find good decision tree's since it is not feasible to generate and test all of them. One algorithm  which in practise terminates with few iterations even on large training sets is ID3. It works as follow: It picks a subset of the training set by random. We call this set the window. It creates a decision tree from this set and test the remaining data set on this decision tree. If there are no classification errors, the algorithm terminate. Otherwise, some of the error classification objects are added to the window and a new test is build and the remaining part of the training set is tested on the tree. This process is repeated until there are no errors. A correct decision tree can be found after few iterations even if there are 50 attributes and up to 30 000 objects in the training set with ID3. However, ID3 might overlook the optimal tree. Furthermore, ID3 may classify objects wrong until the complete training set are included in the window in theory. However, this has ever occurred in practise\cite{Quinlan1986InductionTrees}.


In the case that all objects in a node belongs to the same class, the node should be a leaf in the tree since there is no point of dividing the objects into separate nodes. Other cases are more complex and some  probability calculations are needed to find the best local split, i.e the best split in the current node without considering further splits. If the attribute $A$ has $c_1,\dots c_m$ classes and each each outcome $o_i$ occur approximately $\frac{1}{mn}$ times where n is the size of the training data. Splitting a node on the attribute $A$ is then a poor choice since the attribute gives very little information. Consider the other extreme case where the outcome for $o_i$ for some attribute value $A_i$ on attribute $A$ is almost n. In this case is the value $A_i$ very good to split on since it separate $o_i$ from all the other outcomes. There are many formula's that finds which attributes that holds a lot of information and which split that leads to the purest children. A pure node is a node where all objects belong to the same class. Nodes of the node that is being considering to be divided. 

When a decision tree is constructed, one can construct the tree so that it first stops splitting a branch when all leaf node in a branch have only object of the same class. However, this can lead to overfitting. Overfitting is when a model has divided the training set is so many parts that the model describes not only the underlying structure of the model but also data that correlation by random with some class. Assume that all people older than 70  with a weight over 90kg are sick and we would like to classify a  person older than 70 but has a weight of only 89kg. If the decision tree would have been build so that all leaf nodes in the decision tree where pour, the decision tree may classify the person as healthy since he is not older than 70  and does not weight 90kg. However, one kg might not be that important so it is likely that this person should be classified as sick as well. A solution to this problem is to stop splitting nodes if they have less than some constant number of objects and classify new objects to the majority in the leaf nodes they end up in.

Unknown values in the training set is another problem when decision tree's shall be constructed. If a value is missing for some object, there are two options: throw away the object or try to predict what the the missing value should be. It is easier to throw away objects than to predict attribute values but if many values are missing, the remaining part of the trainng set may be to small to build a good tree. It could also be the case that all objects that are missing values is because the persons do not wanna answer the question. If all people that are drinkers do not answer the question, then there is a serious problem with the data set since alkohol could be a important factor for disease classification. If that is the case, then the model could do many classification errors. Therefore it is preferable to predict empty values instead of throwing away objects.
\begin{equation} \label{equ:repair}
    prob(A = A_i | class = P ) = \frac{prob(A = A_i \& class = P}{pro(class = P)}
\end{equation}
With equation \ref{equ:repair} where $A$ is some attribute and $A_i$ a value attribute $A$ can have can the probability that the attribute $A$ should have the value $A_i$ by looking at the other objects in the training set be calculated. The most likely value can be chosen.

Unknown values are not only a problem when a decision tree shall be constructed but also when an object shall be classified. If we try to classify a person in our model but the attribute age is unknown, we do do not know which branch to follow in the tree. A solution is to give the algorithm an additional input in the case that the object that is to be classified. The extra input, referred to as the token gives a probability how important each branch is. In the case that an attribute value from an object so it is unknown which branch to follow, both branches are followed. Now, the object will end up in several goal nodes and might get classified with a different class in those leafs. To choose one class is each branch that was followed given a probability which symbolise how important that branch is, the token values are summed for each class and the class with the highest sum is chosen. Of course, if two classes are close to make a tie, the confident of the classification is very low.
\bibliographystyle{unsrt}
\bibliography{mendeley}


\end{document}
